{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatization\n",
        "Feb 02, 2024\n",
        "Bhuvana Kanakam"
      ],
      "metadata": {
        "id": "kA9CKOPVlVrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "y1KwrCFhau48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7DobfpPYRxk",
        "outputId": "2f2d9c47-0d9a-489f-da98-26d5f312b417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a porter stemmer and lemmatizer object"
      ],
      "metadata": {
        "id": "3TJq8aNpZShK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "_k9YVEqyZPwb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Words to be stemmed"
      ],
      "metadata": {
        "id": "ON9MayhNZWFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"painting\", \"walking\", \"dressing\", \"likely\", \"children\", \"whom\", \"good\", \"ate\", \"fishing\"]"
      ],
      "metadata": {
        "id": "vbA4UayXZYMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply stemming to each word"
      ],
      "metadata": {
        "id": "bQcXAH3SZeZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_words = [ps.stem(word) for word in words]"
      ],
      "metadata": {
        "id": "cPy38POlZZUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply lemmatizing to each word"
      ],
      "metadata": {
        "id": "W1ydqD3caPUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "SggkyiuCaRwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print original word and stemmed word"
      ],
      "metadata": {
        "id": "MEDf7kmUZgy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for original, stemmed, lemmatized in zip(words, stemmed_words, lemmatized_words):\n",
        "    print(f\"{original} -> Stemmed: {stemmed}, Lemmatized: {lemmatized}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5ln7c_VZgHj",
        "outputId": "5d4d37c0-b89f-4dd2-c4ec-efdbd3fd6463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running -> Stemmed: run, Lemmatized: running\n",
            "painting -> Stemmed: paint, Lemmatized: painting\n",
            "walking -> Stemmed: walk, Lemmatized: walking\n",
            "dressing -> Stemmed: dress, Lemmatized: dressing\n",
            "likely -> Stemmed: like, Lemmatized: likely\n",
            "children -> Stemmed: children, Lemmatized: child\n",
            "whom -> Stemmed: whom, Lemmatized: whom\n",
            "good -> Stemmed: good, Lemmatized: good\n",
            "ate -> Stemmed: ate, Lemmatized: ate\n",
            "fishing -> Stemmed: fish, Lemmatized: fishing\n"
          ]
        }
      ]
    }
  ]
}