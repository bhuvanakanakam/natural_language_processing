{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJsMDZqlwUZx"
      },
      "source": [
        "## Implement word2vec and fasttext Model. RNN\n",
        "### Bhuvana Kanakam, SE21UCSE035\n",
        "\n",
        "#### Problem Statement :\n",
        "Training a RNN model on the given dataset.\n",
        "\n",
        "#### Dataset:\n",
        "https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "#### Implementation: [40 marks]\n",
        "1. Implement the Word2vec model and train the word vectors using\n",
        "skip-gram model with negative sampling.\n",
        "2. Implement the FastText model and train the word vectors [https://github.com/facebookresearch/fastText].\n",
        "Hint: Make use of only “train” folder for training your word vectors.\n",
        "3. You can use “test” folder and sentiment labels, i.e., pos and neg for\n",
        "your sentiment classification task using RNN.\n",
        "4. After creating word vectors using the methods provided above,\n",
        "train your RNN model on the sentiment classification task by making\n",
        "using of these word vectors.\n",
        "\n",
        "#### Results and analysis [25 marks]\n",
        "Present the results of your experiments including performance metrics for\n",
        "each word vector technique used for sentiment classification task.\n",
        "5. Make use of tables, graphs to compare results visually.\n",
        "6. Discuss any findings and report all the hyperparameters for each\n",
        "technique used during experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Px9LS7ciK4u",
        "outputId": "1bf8db36-b50c-424d-ad04-6b77ed9a5693"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbXvauRpob8U"
      },
      "source": [
        "#### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7A0qUsjoer8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NVLc1kyZAA9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8alUy8sYNBF"
      },
      "source": [
        "## Import Data Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQszC82neCw_"
      },
      "source": [
        "#### Mount Google Drive\n",
        "My Drive -> data.zip -> data -> train, test -> pos,neg -> files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyYamMYFYO9G",
        "outputId": "45c0d466-8dc1-447c-cb92-c7b80c609140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvg-SWZKeO50"
      },
      "source": [
        "#### Extract The Data\n",
        "-  unzip the data.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjMmuhegap5r"
      },
      "outputs": [],
      "source": [
        "zip_file_path = '/content/drive/My Drive/data.zip'\n",
        "extracted_folder_path = '/content/data'\n",
        "\n",
        "os.makedirs(extracted_folder_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZpPzgEJe7Oe"
      },
      "source": [
        "- the contents of the extracted folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilIOMpnBi4Fs",
        "outputId": "e2271450-1f64-45d6-f42f-6634ff8ad3ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contents of Extracted Folder:\n",
            "['data', 'data.zip.download', '__MACOSX']\n"
          ]
        }
      ],
      "source": [
        "print(\"Contents of Extracted Folder:\")\n",
        "print(os.listdir(extracted_folder_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYxG_pJKfLbL"
      },
      "source": [
        "#### Define Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mz60KU1jkVCi"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(extracted_folder_path, 'data', 'train')\n",
        "test_dir = os.path.join(extracted_folder_path, 'data', 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdMdlL75aeMx"
      },
      "outputs": [],
      "source": [
        "train_pos = os.path.join(train_dir, 'pos')\n",
        "train_neg = os.path.join(train_dir, 'neg')\n",
        "test_pos = os.path.join(test_dir, 'pos')\n",
        "test_neg = os.path.join(test_dir, 'neg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxh0lIgxfBrv"
      },
      "source": [
        "-  the first 2 files in the train/pos directory, just as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpaZ1tNGb879",
        "outputId": "a7df3004-e215-4e2d-bf06-342aa7708f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Training Data - Positive:\n",
            "['25_7.txt', '5363_9.txt']\n",
            "\n",
            "Extracted Training Data - Negative:\n",
            "['5701_1.txt', '3462_1.txt']\n",
            "\n",
            "Extracted Testing Data - Positive:\n",
            "['12442_10.txt', '5375_7.txt']\n",
            "\n",
            "Extracted Testing Data - Negative:\n",
            "['8160_3.txt', '6931_1.txt']\n"
          ]
        }
      ],
      "source": [
        "print(\"Extracted Training Data - Positive:\")\n",
        "print(os.listdir(train_pos)[:2])\n",
        "\n",
        "print(\"\\nExtracted Training Data - Negative:\")\n",
        "print(os.listdir(train_neg)[:2])\n",
        "\n",
        "print(\"\\nExtracted Testing Data - Positive:\")\n",
        "print(os.listdir(test_pos)[:2])\n",
        "\n",
        "print(\"\\nExtracted Testing Data - Negative:\")\n",
        "print(os.listdir(test_neg)[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwdc1xpDf-F0"
      },
      "source": [
        "## Load and preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kunyJkuUgOtB"
      },
      "source": [
        "#### Read Reviews\n",
        "*Reads the text files in a given directory ('pos' or 'neg') and appends them to a list.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j47z3eAyfPwl"
      },
      "outputs": [],
      "source": [
        "def read_reviews(path, sentiment):\n",
        "    reviews = []\n",
        "    directory = os.path.join(path, sentiment)\n",
        "    for filename in os.listdir(directory):\n",
        "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "            reviews.append(file.read())\n",
        "    return reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALHvtRl-ksE5"
      },
      "source": [
        "- Read reviews from positive and negative directories in training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvzGvQr1fRuF"
      },
      "outputs": [],
      "source": [
        "train_pos_reviews = read_reviews(train_dir, 'pos')\n",
        "train_neg_reviews = read_reviews(train_dir, 'neg')\n",
        "test_pos_reviews = read_reviews(test_dir, 'pos')\n",
        "test_neg_reviews = read_reviews(test_dir, 'neg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFRp_JCNf6oQ"
      },
      "source": [
        "- Combine positive and negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWzW5zG_fTnI"
      },
      "outputs": [],
      "source": [
        "train_texts = train_pos_reviews + train_neg_reviews\n",
        "train_labels = [1] * len(train_pos_reviews) + [0] * len(train_neg_reviews)\n",
        "test_texts = test_pos_reviews + test_neg_reviews\n",
        "test_labels = [1] * len(test_pos_reviews) + [0] * len(test_neg_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEmQ72kXfzMk"
      },
      "source": [
        "- Total Number of Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2X2wIzPfcgF",
        "outputId": "08a0885a-6ff0-4f30-88a3-681c98fcb10a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 25000\n",
            "Number of testing samples: 25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training samples:\", len(train_texts))\n",
        "print(\"Number of testing samples:\", len(test_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvAa6zLufslB"
      },
      "source": [
        "- Print Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "En_EvPqLfuL-",
        "outputId": "23c04df2-160f-40ca-9052-261ee1659e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Positive Reviews:\n",
            "I never saw this when I was a kid, so this was seen with fresh eyes. I had never heard of it and rented it for my 5 year old daughter. Plus, the idea of Christopher Walken singing and dancing made me curious. The special fx are cheesy and the singing and dancing is mediocre. But the story is great. My daughter was entranced. I loved watching Walken in this role thinking about what the future held for him. Very amusing to see him dance! And if the songs weren't great, at least they weren't Disney over-produced saccharine sweetness. The ogre scene in the beginning was a little scary for her, and she was a little nervous when we saw him again at the end, but it was mostly benign. Interestingly, we had recently read \"Puss in Boots\", and I had wondered about the implausibility of the story. But while staying true to almost every aspect, Walken's acting made it believable. Great fun. I'd watch it again with my daughter.\n",
            "This is a classic action flick from the '80s featuring Arnold Schwarzenegger in one of his most memorable roles. Set in a futuristic police state where the government controls everything, including the television networks. One of their most popular TV shows is \"The Running Man\", where convicted felons are hunted down and killed for the entertainment of millions. It's set up like a game show, where the audience votes for their favorite \"stalkers\", trained killers who hunt down and kill the show's unlucky \"contestants\". Audience members also win prizes for correctly predicting who will be killed by whom. And the host is played by none other than Family Feud's Richard Dawson, who's game show experience makes him well suited for this role. When Ben Richards (Arnold) is falsely accused of mass murder, he is forced to play this sadistic game.<br /><br />This movie is chock full of classic Arnold one-liners, such as his famous \"I'll be back\" right before he enters the arena. And he taunts a stalker armed with a flamethrower with \"How about a light?\" I could go on and on, but I don't want to spoil the movie. It's funny stuff!<br /><br />Whether it was intended or not, this movie serves as a great parody of today's \"Reality TV\" craze. Already there are numerous programs that show people enduring pain and humiliation for the entertainment of viewers, and even court cases are televised for their \"entertainment value\". Running Man demonstrates what would happen if reality TV hit rock bottom, and it is a scary picture. One can only hope that the networks have the common sense not to let it go that far.<br /><br />Overall, this is a fun film & I highly recommend it. 9 out of 10!\n",
            "In another one of Bugs Bunny's hare-raisingly wacky shorts, the famous leporid* works in a department store display case, when owner Gildersleeve decides to stuff him. Of course, this proves nearly impossible, as Bugs apparently knows the store better than Gildersleeve (and knows when to cross-dress). As always, they keep everything coming at top speed, and so you have to wonder how hilarious this cartoon must have seemed when it first debuted! Among other things, \"Hare Conditioned\" is a fine example of how the Looney Tunes looked in the '40s before the Termite Terrace crowd polished them. But don't get me wrong, the cartoons were still really good after the refined forms arrived.<br /><br />Anyway, this is a great one.<br /><br />*Leporids are rabbits and hares.\n",
            "\n",
            "Sample Negative Reviews:\n",
            "And so the great rewriting of history continues Hollywood style.<br /><br />This was senseless ridiculous rubbish.<br /><br />Its shocks me that such an amazing amount of money can be spent to produce what is the most contrived, poorly acted inaccurate film I have ever seen. It is appalling.<br /><br />Nic Cage's brief flirtation with serious acting appears to be over. I can only assume that Leaving Las Vegas was a glitch in an otherwise litany of dreadful films.<br /><br />Diane Kruger proves that her performance in Troy was no fluke, she really can't act.<br /><br />Harvey Keitel should be ashamed of himself for working on such tripe.<br /><br />Only recommended for those either recovering from a recent lobotomy or people of an opinion that America invented the world.\n",
            "Slash flicks come few and far between now a days, so when I heard about Cut I had high hopes and heard good things about it. Those good things I heard were all wrong..very wrong! This flick is bad and I mean BAD. It's just plain stupid. Everything about it. Especially the killer's origin and how he stays alive and how he is taken care of in the end. There is nothing original or outstanding about this flick. Just another slasher wannabe with those \"Hip,\" \"Self aware.\" \"Movie savvy\" characters. I'm so sick of that crap. Someone do something different cause the slash genre needs new blood, literally.\n",
            "I thought the movie was a poor documentary. Nothing of substance was discussed. It seemed to cheapen the ideas and did not provide anything new. The film lacked wonder or romance or anything that would really drive one to science. Most scientists appeared \"stereotyped\" and sometimes weird. A woman said that her awards didn't matter a whole lot, only children that were helped. She said that after a 10 minute scene where she explained all her awards. Playing \"humble scientist\", are we? \"I have equations dancing in my head,\" another said. I don't see how that explains anything to us. It hasn't covered significant effects of science on our culture. Politics of science were barely touched.<br /><br />Not a bad flick for a 10-14 year-olds. Other than that, I felt it was boring and unrevealing.<br /><br />4/10\n"
          ]
        }
      ],
      "source": [
        "print(\"Sample Positive Reviews:\")\n",
        "for i in range(3):\n",
        "    print(train_pos_reviews[i])\n",
        "\n",
        "print(\"\\nSample Negative Reviews:\")\n",
        "for i in range(3):\n",
        "    print(train_neg_reviews[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H-p3dmkgIJS"
      },
      "source": [
        "## Train Word Embedding Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO0nJ6i1fut7"
      },
      "outputs": [],
      "source": [
        "word2vec_model = Word2Vec(sentences=[text.split() for text in train_texts], vector_size=100, window=5, min_count=1, sg=1, negative=5)\n",
        "fasttext_model = FastText(sentences=[text.split() for text in train_texts], vector_size=100, window=5, min_count=1, sg=1, negative=5)\n",
        "\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "fasttext_model.save(\"fasttext.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF54dnRpgduc"
      },
      "source": [
        "- *Here, I used the Word2Vec and FastText classes from gensim.models to train my models. `sentences=[text.split() for text in train_texts]:` Tokenizes each review into a list of words for training. The parameters are `vector_size` which tells the dimensionality of the word vectors. `window`, tels the maximum distance between the current and predicted word within a sentence. `min_count`, which ignores all words with a total frequency lower than this `sg`, training algorithm (skip-gram if 1, otherwise CBOW). And `negative`, tells the number of negative samples. I then save the trained models as \"word2vec.model\" and \"fasttext.model\".*\n",
        "- Here, I also observed that the model is taking a lot of time to train. And it is because of the larger dataset from our previous output of 25000 each. Training Word2Vec and FastText models involves iterating over the text data multiple times to learn the embeddings, and this process can be time-consuming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uofGyZY6lt-s"
      },
      "source": [
        "## Data Set Class and Data Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNJoT1dc3nLp"
      },
      "source": [
        "### Data Set Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQqi95Egg_8I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, model):\n",
        "        self.labels = labels\n",
        "        self.texts = [self.text_to_tensor(text, model) for text in texts]\n",
        "\n",
        "    def text_to_tensor(self, text, model):\n",
        "        embeddings = [model.wv[word] for word in text.split() if word in model.wv]\n",
        "        embeddings = np.array(embeddings)  # Convert list of numpy arrays to a single numpy array\n",
        "        return torch.tensor(embeddings, dtype=torch.float)\n",
        "\n",
        "    @staticmethod\n",
        "    def pad_sequence(sequences):\n",
        "        max_len = max([s.size(0) for s in sequences])\n",
        "        padded_sequences = torch.zeros(len(sequences), max_len, sequences[0].size(1))\n",
        "        for i, sequence in enumerate(sequences):\n",
        "            end = sequence.size(0)\n",
        "            padded_sequences[i, :end, :] = sequence[:]\n",
        "        return padded_sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        texts, labels = zip(*batch)\n",
        "        texts = SentimentDataset.pad_sequence(texts)\n",
        "        labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1)  # Add unsqueeze(1) to make it [batch_size, 1]\n",
        "        return texts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8hxfq0XHfUe"
      },
      "source": [
        "#### Dataset and Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpbfHFxul2ts"
      },
      "outputs": [],
      "source": [
        "train_dataset = SentimentDataset(train_texts, train_labels, word2vec_model)\n",
        "test_dataset = SentimentDataset(test_texts, test_labels, word2vec_model)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=SentimentDataset.collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=SentimentDataset.collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8V88hAgme8l"
      },
      "source": [
        "## Define RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx9mGRIQl6Z1"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.fc(x)\n",
        "        return torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BdgVYb2ox7Y"
      },
      "outputs": [],
      "source": [
        "model = RNNModel(100, 256, 1)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgJV0lbZovE6"
      },
      "outputs": [],
      "source": [
        "def log_performance(model_name, accuracy, loss):\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Loss\": loss\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRPlLDgt3ynm"
      },
      "source": [
        "## Function For Model Comparision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52ooEVwlo0O1"
      },
      "outputs": [],
      "source": [
        "def plot_comparison(results):\n",
        "    df = pd.DataFrame(results)\n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Model')\n",
        "    ax1.set_ylabel('Accuracy', color=color)\n",
        "    ax1.bar(df['Model'], df['Accuracy'], color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Loss', color=color)\n",
        "    ax2.plot(df['Model'], df['Loss'], color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    plt.title('Comparison of Word2Vec and FastText Models')\n",
        "    plt.show()\n",
        "    fig.savefig('model_comparison.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj-IOXyRo5p0"
      },
      "outputs": [],
      "source": [
        "results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZAEMBkBmqXz"
      },
      "source": [
        "### Train The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzvI5ucmmrta"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, loss_function, model_name):\n",
        "    model.train()\n",
        "    for epoch in range(5):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for i, (texts, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(texts)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct = (predicted[:, 0] == labels.squeeze()).sum().item()\n",
        "            total_correct += correct\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        accuracy = total_correct / total_samples\n",
        "        print(f'Epoch [{epoch+1}/5], Loss: {avg_loss:.4f}, Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "        test_accuracy = test(model, test_loader)\n",
        "        print(f'Epoch [{epoch+1}/5], Test Accuracy: {test_accuracy*100:.2f}%')\n",
        "\n",
        "        results.append(log_performance(model_name, test_accuracy * 100, avg_loss))\n",
        "\n",
        "    return test_accuracy * 100, avg_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cmqk7ICm0j4"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            outputs = model(texts)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted[:, 0] == labels.squeeze()).sum().item()\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m_f-NeXH1Jy",
        "outputId": "af26722c-ac21-43b0-ccaa-a8190734a641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 0.6938, Accuracy: 49.70%\n",
            "Epoch [1/5], Test Accuracy: 49.64%\n",
            "Epoch [2/5], Loss: 0.6966, Accuracy: 50.08%\n",
            "Epoch [2/5], Test Accuracy: 49.21%\n",
            "Epoch [3/5], Loss: 0.6953, Accuracy: 50.38%\n",
            "Epoch [3/5], Test Accuracy: 49.27%\n",
            "Epoch [4/5], Loss: 0.6947, Accuracy: 49.96%\n",
            "Epoch [4/5], Test Accuracy: 49.25%\n",
            "Epoch [5/5], Loss: 0.6953, Accuracy: 49.78%\n",
            "Epoch [5/5], Test Accuracy: 50.11%\n",
            "Word2Vec - Test Accuracy: 50.11% Loss: 0.69535\n"
          ]
        }
      ],
      "source": [
        "word2vec_model = RNNModel(100, 128, 1)\n",
        "optimizer = torch.optim.Adam(word2vec_model.parameters(), lr=0.001)\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "results = []\n",
        "train_accuracy, train_loss = train(word2vec_model, train_loader, optimizer, loss_function, \"Word2Vec\")\n",
        "print(f'Word2Vec - Test Accuracy: {train_accuracy:.2f}% Loss: {train_loss:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbprEhvHpJw0"
      },
      "outputs": [],
      "source": [
        "fasttext_train_loader = DataLoader(fasttext_train_dataset, batch_size=64, shuffle=True, collate_fn=SentimentDataset.collate_fn)\n",
        "fasttext_test_loader = DataLoader(fasttext_test_dataset, batch_size=64, shuffle=False, collate_fn=SentimentDataset.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UexW7A2XH_4n"
      },
      "outputs": [],
      "source": [
        "fasttext_model = RNNModel(100, 128, 1)\n",
        "optimizer = torch.optim.Adam(fasttext_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCY3wFxzpOs3"
      },
      "outputs": [],
      "source": [
        "train_accuracy, train_loss = train(fasttext_model, fasttext_train_loader, \"FastText\")\n",
        "print(f'FastText - Test Accuracy: {train_accuracy:.2f}% Loss: {train_loss:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE9s1w1rpR74"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcX0cvYmpTHl"
      },
      "outputs": [],
      "source": [
        "plot_comparison(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
